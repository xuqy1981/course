{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and debugging for MindSpore\n",
    "## Target\n",
    "- Be familiared with basic functions of MindInsight.\n",
    "- Based on implementation of Lenet model, learn how to utilize MindInsight to debug training problems.\n",
    "\n",
    "## Setup\n",
    "- MindSpore version: 0.5.0\n",
    "- MindInsight version: 0.7.0\n",
    "- System: Linux Ubuntu 18.04\n",
    "- Hardware: Ascend, GPU or CPU\n",
    "- More information, please refer to:\n",
    "https://www.mindspore.cn/install/en\n",
    "\n",
    "## Experiment\n",
    "- Model: Lenet5 (Please refer to https://gitee.com/mindspore/mindspore/tree/r0.5/model_zoo/lenet)\n",
    "- Dataset: MNIST (Hand written digits, Download: http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.transforms.vision.c_transforms as CV\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.dataset.transforms.vision import Inter\n",
    "from mindspore.common import dtype as mstype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
    "                   num_parallel_workers=1):\n",
    "    \"\"\"\n",
    "    create dataset for train or test\n",
    "    \"\"\"\n",
    "    # define dataset\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # define map operations\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)  # Bilinear mode\n",
    "    rescale_op = CV.Rescale(rescale, shift)\n",
    "    hwc2chw_op = CV.HWC2CHW()\n",
    "    rotation_op = CV.RandomRotation(degrees=200) # modify it to 10 can recover training\n",
    "    type_cast_op = C.TypeCast(mstype.int32)\n",
    "\n",
    "    # apply map operations on images\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"label\", operations=type_cast_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=resize_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=rescale_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=rotation_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=hwc2chw_op, num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    # apply DatasetOps\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)  # 10000 as in LeNet train script\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)\n",
    "\n",
    "    return mnist_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LeNet.\"\"\"\n",
    "import mindspore.nn as nn\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"weight initial for conv layer\"\"\"\n",
    "    weight = weight_variable()\n",
    "    return nn.Conv2d(in_channels, out_channels,\n",
    "                     kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                     weight_init=weight, has_bias=False, pad_mode=\"valid\")\n",
    "\n",
    "\n",
    "def fc_with_initialize(input_channels, out_channels):\n",
    "    \"\"\"weight initial for fc layer\"\"\"\n",
    "    weight = weight_variable()\n",
    "    bias = weight_variable()\n",
    "    return nn.Dense(input_channels, out_channels, weight, bias)\n",
    "\n",
    "\n",
    "def weight_variable():\n",
    "    \"\"\"weight initial\"\"\"\n",
    "    return TruncatedNormal(0.02)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "In the next model structure, you can add the line just after first conv layer in construct method to\n",
    "recover the training process.\n",
    ">>> x = self.relu(x)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"\n",
    "    Lenet network\n",
    "\n",
    "    Args:\n",
    "        num_class (int): Num classes. Default: 10.\n",
    "        channel (int): Num channels. Default: 1.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor\n",
    "    Examples:\n",
    "        >>> LeNet(num_class=10, channel=1)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_class=10, channel=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv1 = conv(channel, 6, 5)\n",
    "        self.conv2 = conv(6, 16, 5)\n",
    "        self.fc1 = fc_with_initialize(16 * 5 * 5, 120)\n",
    "        self.fc2 = fc_with_initialize(120, 84)\n",
    "        self.fc3 = fc_with_initialize(84, self.num_class)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training hyperparameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = edict({\n",
    "    'num_classes': 10,\n",
    "    'lr': 0.001, # modify it to 0.01 to recover the training process\n",
    "    'momentum': 0.9,\n",
    "    'epoch_size': 3,\n",
    "    'batch_size': 32,\n",
    "    'buffer_size': 1000,\n",
    "    'image_height': 32,\n",
    "    'image_width': 32,\n",
    "    'save_checkpoint_steps': 1875,\n",
    "    'keep_checkpoint_max': 10,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training module initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mindspore import context\n",
    "from mindspore.train.callback import LossMonitor, TimeMonitor, SummaryCollector\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\") # modify it according to your own device\n",
    "                                                                  # such as (\"Ascend\", \"GPU\" or \"CPU\")\n",
    "\n",
    "data_path = \"MNIST\" # modify according to your path where the MNIST dataset is saved\n",
    "\n",
    "ds_train = create_dataset(os.path.join(data_path, \"train\"),\n",
    "                              cfg.batch_size,\n",
    "                              cfg.epoch_size)\n",
    "\n",
    "network = LeNet5(cfg.num_classes)\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(is_grad=False, sparse=True, reduction=\"mean\")\n",
    "net_opt = nn.Momentum(network.trainable_params(), cfg.lr, cfg.momentum)\n",
    "time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())\n",
    "# The main object collecting log for visualization\n",
    "summary_collector = SummaryCollector(summary_dir='./summary_dir', collect_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "\n",
    "model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "model.train(cfg['epoch_size'], ds_train, callbacks=[time_cb, \n",
    "                                                    LossMonitor(per_print_times=ds_train.get_dataset_size()), \n",
    "                                                    summary_collector],\n",
    "                dataset_sink_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eval = create_dataset(os.path.join(data_path, \"test\"),\n",
    "                             cfg.batch_size,\n",
    "                             1)\n",
    "acc = model.eval(ds_eval, dataset_sink_mode=False)\n",
    "print('Metrics: ', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
