{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target result (x_1, x_2) = (3, 0.5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as plt_cl\n",
    "\n",
    "# ----------Define the objective function„ÄÅthe partial derivative function,and draw the objective function-----------\n",
    "# Define the objective functionbeale\n",
    "def beale(x1, x2):\n",
    "    return (1.5 - x1 + x1 * x2) ** 2 + (2.25 - x1 + x1 * x2 ** 2) ** 2 + (2.625 - x1 + x1 * x2 ** 3) ** 2\n",
    "\n",
    "# Define the partial derivative function of beale\n",
    "def dbeale_dx(x1, x2):\n",
    "    dfdx1 = 2 * (1.5 - x1 + x1 * x2) * (x2 - 1) + 2 * (2.25 - x1 + x1 * x2 ** 2) * (x2 ** 2 - 1) + 2 * (\n",
    "            2.625 - x1 + x1 * x2 ** 3) * (x2 ** 3 - 1)\n",
    "    dfdx2 = 2 * (1.5 - x1 + x1 * x2) * x1 + 2 * (2.25 - x1 + x1 * x2 ** 2) * (2 * x1 * x2) + 2 * (\n",
    "            2.625 - x1 + x1 * x2 ** 3) * (3 * x1 * x2 ** 2)\n",
    "    return dfdx1, dfdx2\n",
    "\n",
    "step_x1, step_x2 = 0.2, 0.2\n",
    "X1, X2 = np.meshgrid(np.arange(-5, 5 + step_x1, step_x1),\n",
    "                     np.arange(-5, 5 + step_x2, step_x2))\n",
    "Y = beale(X1, X2)\n",
    "print(\"Target result (x_1, x_2) = (3, 0.5)\")\n",
    "\n",
    "# Define the plot function\n",
    "def gd_plot(x_traj):\n",
    "    plt.rcParams['figure.figsize'] = [6, 6]\n",
    "    plt.contour(X1, X2, Y, levels=np.logspace(0, 6, 30),\n",
    "                norm=plt_cl.LogNorm(), cmap=plt.cm.jet)\n",
    "    plt.title('2D Contour Plot of Beale function(Momentum)')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.axis('equal')\n",
    "    plt.plot(3, 0.5, 'k*', markersize=10)\n",
    "    if x_traj is not None:\n",
    "        x_traj = np.array(x_traj)\n",
    "        plt.plot(x_traj[:, 0], x_traj[:, 1], 'k-')\n",
    "    plt.show()\n",
    "    \n",
    "gd_plot(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------adagrad-----------------------------\n",
    "def gd_adagrad(df_dx, x0, conf_para=None):\n",
    "    if conf_para is None:\n",
    "        conf_para = {}\n",
    "    conf_para.setdefault('n_iter', 1000)  # The number of iterations\n",
    "    conf_para.setdefault('learning_rate', 0.001)  # Learning rate\n",
    "    conf_para.setdefault('epsilon', 1e-7)\n",
    "    x_traj = []\n",
    "    x_traj.append(x0)\n",
    "    r = np.zeros_like(x0)\n",
    "    for iter in range(1, conf_para['n_iter'] + 1):\n",
    "        dfdx = np.array(df_dx(x_traj[-1][0], x_traj[-1][1]))\n",
    "        r += dfdx ** 2\n",
    "        x_traj.append(x_traj[-1] - conf_para['learning_rate'] / (np.sqrt(r) + conf_para['epsilon']) * dfdx)\n",
    "    return x_traj\n",
    "\n",
    "x0 = np.array([1.0, 1.5])\n",
    "conf_para_adag = {'n_iter': 500, 'learning_rate': 2}\n",
    "x_traj_adag = gd_adagrad(dbeale_dx, x0, conf_para_adag)\n",
    "print(\"Adagrad reach the extreme point(x_1, x_2) = (%s, %s)\" % (x_traj_adag[-1][0], x_traj_adag[-1][1]))\n",
    "gd_plot(x_traj_adag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore-0.5.0-python3.7-aarch64",
   "language": "python",
   "name": "mindspore-0.5.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
